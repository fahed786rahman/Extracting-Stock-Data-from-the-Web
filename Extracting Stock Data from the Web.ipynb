{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Data of stocks from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#intro)\n",
    "2. [Building a list of companies](#p1)\n",
    "3. [Getting all the Company pricing Data](#p2)\n",
    "4. [Combining All Data for Adj Close Prices](#p3)\n",
    "5. [Additionbal Feature](#p4)\n",
    "6. [Conclusion](#p5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "### Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This programme will be able to gather closing price data of all companies from an index fund. An index fund tracks the overall performance of an entire market index , for example the S&P 100. An index fund is said to provide broad market exposure and has low costs in operation. The index fund builds a portfolio whose holdings is very much the same as the stock market in whole or a large segment of it so it can have same perfomance too. \n",
    "Portfolios of index funds only really change when their benchmark indexes change. Also the managers of the fund can weight the fund to balance put the influence out any single holding in an index or a portfoliio. The pro's of investing in an index fund is you have Diversification, low expense ratios, secure long term returns and is ideal for passive investors.\n",
    "The stock market index the program will be gathering data on will be the S&P 100 which is a stock market index of Unted States stocks mainained by Standards & Poor's. This index was started on June 15, 1983.  The S&P 100 is a subset of the S&P 500 and includes 101 leading U.S stocks with exchange listed options. The companies in the S&P 100 represent about 63% of the market capitilization of S&P 500 and nearly 51% of the  capitilization the America's market in January 2017.The S&P 100 conatins the largest stock  and the most established companies in the S&P 500.\n",
    "\n",
    "The following definitions have been taken from \"https://pythonprogramming.net/getting-stock-prices-python-programming-for-finance/\". :\n",
    "\n",
    "The data the programme will be able to extract will be Open,High,Low,Close,Volume and Adj Close.\n",
    "\n",
    "Open - When the stock market opens in the morning for trading, what was the price of one share?\n",
    "\n",
    "High - over the course of the trading day, what was the highest value for that day?\n",
    "\n",
    "Low - over the course of the trading day, what was the lowest value for that day?\n",
    "\n",
    "Close - When the trading day was over, what was the final price?\n",
    "\n",
    "Volume - For that day, how many shares were traded?\n",
    "\n",
    "Adj Close- Adj Close is helpful, since it accounts for future stock splits, and gives the relative price to splits. For this reason, the adjusted prices are the prices you're most likely to be dealing with.\n",
    "\n",
    "The programme will aim to download end of day data of all companies in the S&P 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p1\"></a>\n",
    "### Building a list of companies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin we need a Python list of companies from the S&P 100. The list of names with their tickers is available on wikipedia: https://en.wikipedia.org/wiki/S%26P_100 .\n",
    "The tickers in Wikipedia are in a table. To handle this we will need to use the HTML parsing library, Beautiful Soup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin program with imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import pickle \n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bs4 is Beautiful Soup, pickle is to help save the list of companies and requests grabs the code from the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will start by defining a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_NASDAQ100_tickers():\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/NASDAQ-100#')\n",
    "    soup = bs.BeautifulSoup(resp.text, \"lxml\")\n",
    "    table = soup.find('table', {'class':'wikitable sortable','id': 'constituents'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first visit the webpage which contains the source code we want. We want the .text attribute which we turn to soup using BeautifulSoup. Beautiful Soup turns source code into a BeautifulSoup object that can now be treated like a Python object. We then find the table which has all the ticker symbols of the stock by searching wikitable sortable classes with constituents for its ID . Ticker symbols are unique series of letters assigned to a stock. For example Amazon has the ticker AMZN. I know how to specify the exact table as I had viewed the source code of the webpage. \n",
    "We then just iterate through the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-da7bae9d5c29>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-da7bae9d5c29>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    for row in table.findAll('tr')[1:]:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text.strip()\n",
    "        tickers.append(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row , apart from the Header row which is why I typed [1:], the code is sayinhg the ticker is the \"table data\" (td), we grab the .text and we append the tickers into our list (named tickers).\n",
    "\n",
    "To save this list we use pickle which serializes Python objects from us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(\"SandP100tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers, f)\n",
    "\n",
    "    print(tickers)\n",
    "\n",
    "    return tickers\n",
    "\n",
    "save_SandP100_tickers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This then saves the tickers into a PICKLE file. we can update this list at any time. The code will print the tickers list just to show it is working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'AIG', 'AMD', 'AMGN', 'AMT', 'AMZN', 'AVGO', 'AXP', 'BA', 'BAC', 'BK', 'BKNG', 'BLK', 'BMY', 'BRK.B', 'C', 'CAT', 'CHTR', 'CL', 'CMCSA', 'COF', 'COP', 'COST', 'CRM', 'CSCO', 'CVS', 'CVX', 'DE', 'DHR', 'DIS', 'DOW', 'DUK', 'EMR', 'EXC', 'F', 'FDX', 'GD', 'GE', 'GILD', 'GM', 'GOOG', 'GOOGL', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KHC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'MA', 'MCD', 'MDLZ', 'MDT', 'MET', 'META', 'MMM', 'MO', 'MRK', 'MS', 'MSFT', 'NEE', 'NFLX', 'NKE', 'NVDA', 'ORCL', 'PEP', 'PFE', 'PG', 'PM', 'PYPL', 'QCOM', 'RTX', 'SBUX', 'SCHW', 'SO', 'SPG', 'T', 'TGT', 'TMO', 'TMUS', 'TSLA', 'TXN', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VZ', 'WFC', 'WMT', 'XOM']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AAPL',\n",
       " 'ABBV',\n",
       " 'ABT',\n",
       " 'ACN',\n",
       " 'ADBE',\n",
       " 'AIG',\n",
       " 'AMD',\n",
       " 'AMGN',\n",
       " 'AMT',\n",
       " 'AMZN',\n",
       " 'AVGO',\n",
       " 'AXP',\n",
       " 'BA',\n",
       " 'BAC',\n",
       " 'BK',\n",
       " 'BKNG',\n",
       " 'BLK',\n",
       " 'BMY',\n",
       " 'BRK.B',\n",
       " 'C',\n",
       " 'CAT',\n",
       " 'CHTR',\n",
       " 'CL',\n",
       " 'CMCSA',\n",
       " 'COF',\n",
       " 'COP',\n",
       " 'COST',\n",
       " 'CRM',\n",
       " 'CSCO',\n",
       " 'CVS',\n",
       " 'CVX',\n",
       " 'DE',\n",
       " 'DHR',\n",
       " 'DIS',\n",
       " 'DOW',\n",
       " 'DUK',\n",
       " 'EMR',\n",
       " 'EXC',\n",
       " 'F',\n",
       " 'FDX',\n",
       " 'GD',\n",
       " 'GE',\n",
       " 'GILD',\n",
       " 'GM',\n",
       " 'GOOG',\n",
       " 'GOOGL',\n",
       " 'GS',\n",
       " 'HD',\n",
       " 'HON',\n",
       " 'IBM',\n",
       " 'INTC',\n",
       " 'JNJ',\n",
       " 'JPM',\n",
       " 'KHC',\n",
       " 'KO',\n",
       " 'LIN',\n",
       " 'LLY',\n",
       " 'LMT',\n",
       " 'LOW',\n",
       " 'MA',\n",
       " 'MCD',\n",
       " 'MDLZ',\n",
       " 'MDT',\n",
       " 'MET',\n",
       " 'META',\n",
       " 'MMM',\n",
       " 'MO',\n",
       " 'MRK',\n",
       " 'MS',\n",
       " 'MSFT',\n",
       " 'NEE',\n",
       " 'NFLX',\n",
       " 'NKE',\n",
       " 'NVDA',\n",
       " 'ORCL',\n",
       " 'PEP',\n",
       " 'PFE',\n",
       " 'PG',\n",
       " 'PM',\n",
       " 'PYPL',\n",
       " 'QCOM',\n",
       " 'RTX',\n",
       " 'SBUX',\n",
       " 'SCHW',\n",
       " 'SO',\n",
       " 'SPG',\n",
       " 'T',\n",
       " 'TGT',\n",
       " 'TMO',\n",
       " 'TMUS',\n",
       " 'TSLA',\n",
       " 'TXN',\n",
       " 'UNH',\n",
       " 'UNP',\n",
       " 'UPS',\n",
       " 'USB',\n",
       " 'V',\n",
       " 'VZ',\n",
       " 'WFC',\n",
       " 'WMT',\n",
       " 'XOM']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import pickle \n",
    "import requests\n",
    "\n",
    "def save_SandP100_tickers():\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/S%26P_100')\n",
    "    soup = bs.BeautifulSoup(resp.text, \"lxml\")\n",
    "    table = soup.find('table', {'class':'wikitable sortable','id': 'constituents'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "        with open(\"SandP100tickers.pickle\",\"wb\") as f:\n",
    "            pickle.dump(tickers, f)\n",
    "\n",
    "    print(tickers)\n",
    "\n",
    "    return tickers\n",
    "\n",
    "save_SandP100_tickers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p2\"></a>\n",
    "### Getting all the Company pricing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The programme will now retrieve all pricing data for each company. First we will need some more imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import os\n",
    "import pandas\n",
    "import pandas_datareader.data as web\n",
    "import pickle\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datetime is needed to specify dates for the Pandas datareader, os is to create and to also check for directories.\n",
    "Then another function is defined:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_yahoo(reload_SandP100=False):\n",
    "    if reload_SandP100:\n",
    "        tickers = save_SandP100_tickers()\n",
    "    else:\n",
    "        with open(\"SandP100tickers.pickle\",\"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program here will re-pull the S&P100 list or will just use our pickle.\n",
    "\n",
    "Then we want the to start saving the data. To do this, a directory needs to be built which will store the data for each company.\n",
    "Therefore a new directory is built:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data needs to be pulled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tickers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ac9f1a68cd22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtickers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tickers' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    start = dt.datetime(2020,1,1)\n",
    "    end = dt.datetime.now()\n",
    "\n",
    "    for ticker in tickers:\n",
    "        \n",
    "        print(ticker)\n",
    "        \n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            try:\n",
    "                df = web.DataReader(ticker, 'yahoo', start, end)\n",
    "                df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "            except Exception as ex:\n",
    "                print('Error:', ex)\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "            \n",
    "get_data_from_yahoo()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created the variables 'start' which is equal to the date (1,1,2020 ) and the variable 'end' is todays date. \n",
    "This means that the code will extract each companies data from the (1,1,2020) up until todays date.\n",
    "\n",
    "The code states if the directory does not contain the information the line web.DataReader(ticker, 'yahoo', start, end) uses pandas_datareader package, looks for the stock for each ticker , gets the information froom yahoo, for the starting date until todays date. \n",
    "This then creates an csv file for each company. The 'except' is there just in case yahoo cannot find the data then it will print 'error' for the ticker. This is done so if this does happen the program does not fail but continues with the next ticker. If the data has already been collected, say this is the 2nd time running the programme, then for each ticker it will print \"Already have\" then whichever ticker the data has already has data retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'AIG', 'AMD', 'AMGN', 'AMT', 'AMZN', 'AVGO', 'AXP', 'BA', 'BAC', 'BK', 'BKNG', 'BLK', 'BMY', 'BRK.B', 'C', 'CAT', 'CHTR', 'CL', 'CMCSA', 'COF', 'COP', 'COST', 'CRM', 'CSCO', 'CVS', 'CVX', 'DE', 'DHR', 'DIS', 'DOW', 'DUK', 'EMR', 'EXC', 'F', 'FDX', 'GD', 'GE', 'GILD', 'GM', 'GOOG', 'GOOGL', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KHC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'MA', 'MCD', 'MDLZ', 'MDT', 'MET', 'META', 'MMM', 'MO', 'MRK', 'MS', 'MSFT', 'NEE', 'NFLX', 'NKE', 'NVDA', 'ORCL', 'PEP', 'PFE', 'PG', 'PM', 'PYPL', 'QCOM', 'RTX', 'SBUX', 'SCHW', 'SO', 'SPG', 'T', 'TGT', 'TMO', 'TMUS', 'TSLA', 'TXN', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VZ', 'WFC', 'WMT', 'XOM']\n",
      "AAPL\n",
      "Error: string indices must be integers\n",
      "ABBV\n",
      "Error: string indices must be integers\n",
      "ABT\n",
      "Error: string indices must be integers\n",
      "ACN\n",
      "Error: string indices must be integers\n",
      "ADBE\n",
      "Error: string indices must be integers\n",
      "AIG\n",
      "Error: string indices must be integers\n",
      "AMD\n",
      "Error: string indices must be integers\n",
      "AMGN\n",
      "Error: string indices must be integers\n",
      "AMT\n",
      "Error: string indices must be integers\n",
      "AMZN\n",
      "Error: string indices must be integers\n",
      "AVGO\n",
      "Error: string indices must be integers\n",
      "AXP\n",
      "Error: string indices must be integers\n",
      "BA\n",
      "Error: string indices must be integers\n",
      "BAC\n",
      "Error: string indices must be integers\n",
      "BK\n",
      "Error: string indices must be integers\n",
      "BKNG\n",
      "Error: string indices must be integers\n",
      "BLK\n",
      "Error: string indices must be integers\n",
      "BMY\n",
      "Error: string indices must be integers\n",
      "BRK.B\n",
      "Error: string indices must be integers\n",
      "C\n",
      "Error: string indices must be integers\n",
      "CAT\n",
      "Error: string indices must be integers\n",
      "CHTR\n",
      "Error: string indices must be integers\n",
      "CL\n",
      "Error: string indices must be integers\n",
      "CMCSA\n",
      "Error: string indices must be integers\n",
      "COF\n",
      "Error: string indices must be integers\n",
      "COP\n",
      "Error: string indices must be integers\n",
      "COST\n",
      "Error: string indices must be integers\n",
      "CRM\n",
      "Error: string indices must be integers\n",
      "CSCO\n",
      "Error: string indices must be integers\n",
      "CVS\n",
      "Error: string indices must be integers\n",
      "CVX\n",
      "Error: string indices must be integers\n",
      "DE\n",
      "Error: string indices must be integers\n",
      "DHR\n",
      "Error: string indices must be integers\n",
      "DIS\n",
      "Error: string indices must be integers\n",
      "DOW\n",
      "Error: string indices must be integers\n",
      "DUK\n",
      "Error: string indices must be integers\n",
      "EMR\n",
      "Error: string indices must be integers\n",
      "EXC\n",
      "Error: string indices must be integers\n",
      "F\n",
      "Error: string indices must be integers\n",
      "FDX\n",
      "Error: string indices must be integers\n",
      "GD\n",
      "Error: string indices must be integers\n",
      "GE\n",
      "Error: string indices must be integers\n",
      "GILD\n",
      "Error: string indices must be integers\n",
      "GM\n",
      "Error: string indices must be integers\n",
      "GOOG\n",
      "Error: string indices must be integers\n",
      "GOOGL\n",
      "Error: string indices must be integers\n",
      "GS\n",
      "Error: string indices must be integers\n",
      "HD\n",
      "Error: string indices must be integers\n",
      "HON\n",
      "Error: string indices must be integers\n",
      "IBM\n",
      "Error: string indices must be integers\n",
      "INTC\n",
      "Error: string indices must be integers\n",
      "JNJ\n",
      "Error: string indices must be integers\n",
      "JPM\n",
      "Error: string indices must be integers\n",
      "KHC\n",
      "Error: string indices must be integers\n",
      "KO\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "def save_SandP100_tickers():\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/S%26P_100')\n",
    "    soup = bs.BeautifulSoup(resp.text, \"lxml\")\n",
    "    table = soup.find('table', {'class':'wikitable sortable','id': 'constituents'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "        with open(\"SandP100tickers.pickle\",\"wb\") as f:\n",
    "            pickle.dump(tickers, f)\n",
    "\n",
    "    print(tickers)\n",
    "\n",
    "    return tickers\n",
    "\n",
    "save_SandP100_tickers()\n",
    "\n",
    "def get_data_from_yahoo(reload_SandP100=False):\n",
    "    if reload_SandP100:\n",
    "        tickers = save_SandP100_tickers()\n",
    "    else:\n",
    "        with open(\"SandP100tickers.pickle\",\"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    \n",
    "    start = dt.datetime(2020,1,1)\n",
    "    end = dt.datetime.now()\n",
    "\n",
    "    for ticker in tickers:\n",
    "        \n",
    "        print(ticker)\n",
    "        \n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            try:\n",
    "                df = web.DataReader(ticker, 'yahoo', start, end)\n",
    "                df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "            except Exception as ex:\n",
    "                print('Error:', ex)\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "            \n",
    "get_data_from_yahoo()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p3\"></a>\n",
    "### Combining All Data for Adj Close Prices \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the data , we only want some of if. Each of the stock files have columns with the prices at: Open,High,Low,Close,Volume, and Adj Close. The aim of this programme is to have all companies Adj Close price.\n",
    "\n",
    "To do this we need to compile all the stock datasets together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_data():\n",
    "    with open(\"SandP100tickers.pickle\",\"rb\") as f:\n",
    "        tickers = pickle.load(f)\n",
    "        \n",
    "    main_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin we extact all our previously made list tickers and begin with an empty Data frame called main_df.\n",
    "Now we can read in each stock's dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for count,ticker in enumerate(tickers):\n",
    "        df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        df.set_index('Date', inplace=True)\n",
    "        \n",
    "    df.rename(columns = {'Adj Close': ticker}, inplace=True)\n",
    "    df.drop(['Open','High','Low','Close','Volume'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code gets all the 'Adj Close' data for each date for each company and drops the other columns.\n",
    "\n",
    "Now the shared dataframe is builts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there's no data in the main_df then we make main_df to df, otherwise Panda's join is used to make on dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this loop there will also be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if count % 10 == 0:\n",
    "            print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will print the count of the tickers left if its divisble by 10. So it will count up to 100 from 0 in 10's.\n",
    "Then to finish this function off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(main_df.head)\n",
    "    main_df.to_csv('SandP100_joined_closes.csv')\n",
    "\n",
    "compile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just transfers the wanted data into a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'AIG', 'AMD', 'AMGN', 'AMT', 'AMZN', 'AVGO', 'AXP', 'BA', 'BAC', 'BK', 'BKNG', 'BLK', 'BMY', 'BRK.B', 'C', 'CAT', 'CHTR', 'CL', 'CMCSA', 'COF', 'COP', 'COST', 'CRM', 'CSCO', 'CVS', 'CVX', 'DE', 'DHR', 'DIS', 'DOW', 'DUK', 'EMR', 'EXC', 'F', 'FDX', 'GD', 'GE', 'GILD', 'GM', 'GOOG', 'GOOGL', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KHC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'MA', 'MCD', 'MDLZ', 'MDT', 'MET', 'META', 'MMM', 'MO', 'MRK', 'MS', 'MSFT', 'NEE', 'NFLX', 'NKE', 'NVDA', 'ORCL', 'PEP', 'PFE', 'PG', 'PM', 'PYPL', 'QCOM', 'RTX', 'SBUX', 'SCHW', 'SO', 'SPG', 'T', 'TGT', 'TMO', 'TMUS', 'TSLA', 'TXN', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VZ', 'WFC', 'WMT', 'XOM']\n",
      "AAPL\n",
      "Error: string indices must be integers\n",
      "ABBV\n",
      "Error: string indices must be integers\n",
      "ABT\n",
      "Error: string indices must be integers\n",
      "ACN\n",
      "Error: string indices must be integers\n",
      "ADBE\n",
      "Error: string indices must be integers\n",
      "AIG\n",
      "Error: string indices must be integers\n",
      "AMD\n",
      "Error: string indices must be integers\n",
      "AMGN\n",
      "Error: string indices must be integers\n",
      "AMT\n",
      "Error: string indices must be integers\n",
      "AMZN\n",
      "Error: string indices must be integers\n",
      "AVGO\n",
      "Error: string indices must be integers\n",
      "AXP\n",
      "Error: string indices must be integers\n",
      "BA\n",
      "Error: string indices must be integers\n",
      "BAC\n",
      "Error: string indices must be integers\n",
      "BK\n",
      "Error: string indices must be integers\n",
      "BKNG\n",
      "Error: string indices must be integers\n",
      "BLK\n",
      "Error: string indices must be integers\n",
      "BMY\n",
      "Error: string indices must be integers\n",
      "BRK.B\n",
      "Error: string indices must be integers\n",
      "C\n",
      "Error: string indices must be integers\n",
      "CAT\n",
      "Error: string indices must be integers\n",
      "CHTR\n",
      "Error: string indices must be integers\n",
      "CL\n",
      "Error: string indices must be integers\n",
      "CMCSA\n",
      "Error: string indices must be integers\n",
      "COF\n",
      "Error: string indices must be integers\n",
      "COP\n",
      "Error: string indices must be integers\n",
      "COST\n",
      "Error: string indices must be integers\n",
      "CRM\n",
      "Error: string indices must be integers\n",
      "CSCO\n",
      "Error: string indices must be integers\n",
      "CVS\n",
      "Error: string indices must be integers\n",
      "CVX\n",
      "Error: string indices must be integers\n",
      "DE\n",
      "Error: string indices must be integers\n",
      "DHR\n",
      "Error: string indices must be integers\n",
      "DIS\n",
      "Error: string indices must be integers\n",
      "DOW\n",
      "Error: string indices must be integers\n",
      "DUK\n",
      "Error: string indices must be integers\n",
      "EMR\n",
      "Error: string indices must be integers\n",
      "EXC\n",
      "Error: string indices must be integers\n",
      "F\n",
      "Error: string indices must be integers\n",
      "FDX\n",
      "Error: string indices must be integers\n",
      "GD\n",
      "Error: string indices must be integers\n",
      "GE\n",
      "Error: string indices must be integers\n",
      "GILD\n",
      "Error: string indices must be integers\n",
      "GM\n",
      "Error: string indices must be integers\n",
      "GOOG\n",
      "Error: string indices must be integers\n",
      "GOOGL\n",
      "Error: string indices must be integers\n",
      "GS\n",
      "Error: string indices must be integers\n",
      "HD\n",
      "Error: string indices must be integers\n",
      "HON\n",
      "Error: string indices must be integers\n",
      "IBM\n",
      "Error: string indices must be integers\n",
      "INTC\n",
      "Error: string indices must be integers\n",
      "JNJ\n",
      "Error: string indices must be integers\n",
      "JPM\n",
      "Error: string indices must be integers\n",
      "KHC\n",
      "Error: string indices must be integers\n",
      "KO\n",
      "Error: string indices must be integers\n",
      "LIN\n",
      "Error: string indices must be integers\n",
      "LLY\n",
      "Error: string indices must be integers\n",
      "LMT\n",
      "Error: string indices must be integers\n",
      "LOW\n",
      "Error: string indices must be integers\n",
      "MA\n",
      "Error: string indices must be integers\n",
      "MCD\n",
      "Error: string indices must be integers\n",
      "MDLZ\n",
      "Error: string indices must be integers\n",
      "MDT\n",
      "Error: string indices must be integers\n",
      "MET\n",
      "Error: string indices must be integers\n",
      "META\n",
      "Error: string indices must be integers\n",
      "MMM\n",
      "Error: string indices must be integers\n",
      "MO\n",
      "Error: string indices must be integers\n",
      "MRK\n",
      "Error: string indices must be integers\n",
      "MS\n",
      "Error: string indices must be integers\n",
      "MSFT\n",
      "Error: string indices must be integers\n",
      "NEE\n",
      "Error: string indices must be integers\n",
      "NFLX\n",
      "Error: string indices must be integers\n",
      "NKE\n",
      "Error: string indices must be integers\n",
      "NVDA\n",
      "Error: string indices must be integers\n",
      "ORCL\n",
      "Error: string indices must be integers\n",
      "PEP\n",
      "Error: string indices must be integers\n",
      "PFE\n",
      "Error: string indices must be integers\n",
      "PG\n",
      "Error: string indices must be integers\n",
      "PM\n",
      "Error: string indices must be integers\n",
      "PYPL\n",
      "Error: string indices must be integers\n",
      "QCOM\n",
      "Error: string indices must be integers\n",
      "RTX\n",
      "Error: string indices must be integers\n",
      "SBUX\n",
      "Error: string indices must be integers\n",
      "SCHW\n",
      "Error: string indices must be integers\n",
      "SO\n",
      "Error: string indices must be integers\n",
      "SPG\n",
      "Error: string indices must be integers\n",
      "T\n",
      "Error: string indices must be integers\n",
      "TGT\n",
      "Error: string indices must be integers\n",
      "TMO\n",
      "Error: string indices must be integers\n",
      "TMUS\n",
      "Error: string indices must be integers\n",
      "TSLA\n",
      "Error: string indices must be integers\n",
      "TXN\n",
      "Error: string indices must be integers\n",
      "UNH\n",
      "Error: string indices must be integers\n",
      "UNP\n",
      "Error: string indices must be integers\n",
      "UPS\n",
      "Error: string indices must be integers\n",
      "USB\n",
      "Error: string indices must be integers\n",
      "V\n",
      "Error: string indices must be integers\n",
      "VZ\n",
      "Error: string indices must be integers\n",
      "WFC\n",
      "Error: string indices must be integers\n",
      "WMT\n",
      "Error: string indices must be integers\n",
      "XOM\n",
      "Error: string indices must be integers\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stock_dfs/AAPL.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f6da2eaddd68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mmain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SandP100_joined_closes.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m \u001b[0mcompile_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-f6da2eaddd68>\u001b[0m in \u001b[0;36mcompile_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mticker\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stock_dfs/{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Jupyter\\envs\\project\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Jupyter\\envs\\project\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Jupyter\\envs\\project\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Jupyter\\envs\\project\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1661\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Jupyter\\envs\\project\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stock_dfs/AAPL.csv'"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "def save_SandP100_tickers():\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/S%26P_100')\n",
    "    soup = bs.BeautifulSoup(resp.text, \"lxml\")\n",
    "    table = soup.find('table', {'class':'wikitable sortable','id': 'constituents'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "        with open(\"SandP100tickers.pickle\",\"wb\") as f:\n",
    "            pickle.dump(tickers, f)\n",
    "\n",
    "    print(tickers)\n",
    "\n",
    "    return tickers\n",
    "\n",
    "save_SandP100_tickers()\n",
    "\n",
    "def get_data_from_yahoo(reload_SandP100=False):\n",
    "    if reload_SandP100:\n",
    "        tickers = save_SandP100_tickers()\n",
    "    else:\n",
    "        with open(\"SandP100tickers.pickle\",\"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    \n",
    "    start = dt.datetime(2020,1,1)\n",
    "    end = dt.datetime.now()\n",
    "\n",
    "    for ticker in tickers:\n",
    "        \n",
    "        print(ticker)\n",
    "        \n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            try:\n",
    "                df = web.DataReader(ticker, 'yahoo', start, end)\n",
    "                df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "            except Exception as ex:\n",
    "                print('Error:', ex)\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "            \n",
    "get_data_from_yahoo()  \n",
    "\n",
    "def compile_data():\n",
    "    with open(\"SandP100tickers.pickle\",\"rb\") as f:\n",
    "        tickers = pickle.load(f)\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for count,ticker in enumerate(tickers):\n",
    "        df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        df.rename(columns = {'Adj Close': ticker}, inplace=True)\n",
    "        df.drop(['Open','High','Low','Close','Volume'], 1, inplace=True)\n",
    "\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df, how='outer')\n",
    "\n",
    "        if count % 10 == 0:\n",
    "            print(count)\n",
    "\n",
    "    print(main_df.head)\n",
    "    main_df.to_csv('SandP100_joined_closes.csv')\n",
    "\n",
    "compile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This programme up until now downloads end of day data of all the companies in the S&P100 index fund and saves it into a csv file. Therefore it fullfills what the aim of the programme was. However the code was imporved upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p4\"></a>\n",
    "### Additional Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional feauture the program will entail is a correlation heat map that shows the correlation between each function. It is important to know the correlation between stocks as it can help show the investor that they may be as diversified as they initially thought. The stocks could be in different sectors but if they are dependent on the same thing then the portfolio is less protected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function starts with more imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib is used to graph things, and we are using a style so the graph is appealing and readable. \n",
    "Now the function starts with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data():\n",
    "    df= pd.read_csv('SandP100_joined_closes.csv')\n",
    "    df_corr = df.corr()\n",
    "    print(df_corr.head())\n",
    "    df_corr.to_csv('SandP100corr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .corr() looks at the whole DataFrame and determines its correlation of every column to every column.\n",
    "The last line just saves the correlation data into a csv file.\n",
    "\n",
    "The next part of the function graphs it instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    data=df_corr.values\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line gives the data that the code will graph, giving a numpy array of values whcih are the correlation numbers. The next part is where the figure and axis are created.\n",
    "Now the heatmap will be created using pcolor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " heatmap= ax.pcolor(data, cmap=plt.cm.RdYlGn)\n",
    "    fig.colorbar(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The colours in the heatmap are RdYlGn , which will be a colour map that goes from red on the low side, yellow for the middle to green for positive.\n",
    "This corresponds to red showing negative correlations, yellow giving for no correlation and green showing positive correlation.\n",
    "\n",
    "Next the code labels the x and yaxis with ticks so we know which correlations are for which companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_xticks(np.arange(data.shape[1]) + 0.5, minor=False)\n",
    "ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will flip the axis so the x axis is on the top and the y is inverted so the data is easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the code wil add the company tickers to each tick as the ticks have no labels on so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_labels = df_corr.columns\n",
    "    row_labels = df_corr.index\n",
    "    ax.set_xticklabels(column_labels)\n",
    "    ax.set_yticklabels(row_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "    heatmap.set_clim(-1,1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "visualize_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part just rotates the xticks by 90 degrees so they appear vertical as we will have 100 ticks, making it easier to read the ticks.\n",
    "The 2nd line tells the graph our range is (-1,1) , this will be already the case but this is just reassurance.\n",
    "This is the function finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the finished programme is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'AIG', 'AMD', 'AMGN', 'AMT', 'AMZN', 'AVGO', 'AXP', 'BA', 'BAC', 'BK', 'BKNG', 'BLK', 'BMY', 'BRK.B', 'C', 'CAT', 'CHTR', 'CL', 'CMCSA', 'COF', 'COP', 'COST', 'CRM', 'CSCO', 'CVS', 'CVX', 'DE', 'DHR', 'DIS', 'DOW', 'DUK', 'EMR', 'EXC', 'F', 'FDX', 'GD', 'GE', 'GILD', 'GM', 'GOOG', 'GOOGL', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KHC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'MA', 'MCD', 'MDLZ', 'MDT', 'MET', 'META', 'MMM', 'MO', 'MRK', 'MS', 'MSFT', 'NEE', 'NFLX', 'NKE', 'NVDA', 'ORCL', 'PEP', 'PFE', 'PG', 'PM', 'PYPL', 'QCOM', 'RTX', 'SBUX', 'SCHW', 'SO', 'SPG', 'T', 'TGT', 'TMO', 'TMUS', 'TSLA', 'TXN', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VZ', 'WFC', 'WMT', 'XOM']\n",
      "AAPL\n",
      "Already have AAPL\n",
      "ABBV\n",
      "Already have ABBV\n",
      "ABT\n",
      "Already have ABT\n",
      "ACN\n",
      "Already have ACN\n",
      "ADBE\n",
      "Already have ADBE\n",
      "AIG\n",
      "Already have AIG\n",
      "AMD\n",
      "Error: string indices must be integers\n",
      "AMGN\n",
      "Already have AMGN\n",
      "AMT\n",
      "Already have AMT\n",
      "AMZN\n",
      "Already have AMZN\n",
      "AVGO\n",
      "Error: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "AXP\n",
      "Already have AXP\n",
      "BA\n",
      "Already have BA\n",
      "BAC\n",
      "Already have BAC\n",
      "BK\n",
      "Already have BK\n",
      "BKNG\n",
      "Already have BKNG\n",
      "BLK\n",
      "Already have BLK\n",
      "BMY\n",
      "Already have BMY\n",
      "BRK.B\n",
      "Error: string indices must be integers\n",
      "C\n",
      "Already have C\n",
      "CAT\n",
      "Already have CAT\n",
      "CHTR\n",
      "Already have CHTR\n",
      "CL\n",
      "Already have CL\n",
      "CMCSA\n",
      "Already have CMCSA\n",
      "COF\n",
      "Already have COF\n",
      "COP\n",
      "Already have COP\n",
      "COST\n",
      "Already have COST\n",
      "CRM\n",
      "Already have CRM\n",
      "CSCO\n",
      "Already have CSCO\n",
      "CVS\n",
      "Already have CVS\n",
      "CVX\n",
      "Already have CVX\n",
      "DE\n",
      "Error: string indices must be integers\n",
      "DHR\n",
      "Already have DHR\n",
      "DIS\n",
      "Already have DIS\n",
      "DOW\n",
      "Already have DOW\n",
      "DUK\n",
      "Already have DUK\n",
      "EMR\n",
      "Already have EMR\n",
      "EXC\n",
      "Already have EXC\n",
      "F\n",
      "Already have F\n",
      "FDX\n",
      "Already have FDX\n",
      "GD\n",
      "Already have GD\n",
      "GE\n",
      "Already have GE\n",
      "GILD\n",
      "Already have GILD\n",
      "GM\n",
      "Already have GM\n",
      "GOOG\n",
      "Already have GOOG\n",
      "GOOGL\n",
      "Already have GOOGL\n",
      "GS\n",
      "Already have GS\n",
      "HD\n",
      "Already have HD\n",
      "HON\n",
      "Already have HON\n",
      "IBM\n",
      "Already have IBM\n",
      "INTC\n",
      "Already have INTC\n",
      "JNJ\n",
      "Already have JNJ\n",
      "JPM\n",
      "Already have JPM\n",
      "KHC\n",
      "Already have KHC\n",
      "KO\n",
      "Already have KO\n",
      "LIN\n",
      "Error: string indices must be integers\n",
      "LLY\n",
      "Already have LLY\n",
      "LMT\n",
      "Already have LMT\n",
      "LOW\n",
      "Already have LOW\n",
      "MA\n",
      "Already have MA\n",
      "MCD\n",
      "Already have MCD\n",
      "MDLZ\n",
      "Already have MDLZ\n",
      "MDT\n",
      "Already have MDT\n",
      "MET\n",
      "Already have MET\n",
      "META\n",
      "Error: string indices must be integers\n",
      "MMM\n",
      "Already have MMM\n",
      "MO\n",
      "Already have MO\n",
      "MRK\n",
      "Already have MRK\n",
      "MS\n",
      "Already have MS\n",
      "MSFT\n",
      "Already have MSFT\n",
      "NEE\n",
      "Already have NEE\n",
      "NFLX\n",
      "Already have NFLX\n",
      "NKE\n",
      "Already have NKE\n",
      "NVDA\n",
      "Already have NVDA\n",
      "ORCL\n",
      "Already have ORCL\n",
      "PEP\n",
      "Already have PEP\n",
      "PFE\n",
      "Already have PFE\n",
      "PG\n",
      "Already have PG\n",
      "PM\n",
      "Already have PM\n",
      "PYPL\n",
      "Already have PYPL\n",
      "QCOM\n",
      "Already have QCOM\n",
      "RTX\n",
      "Already have RTX\n",
      "SBUX\n",
      "Already have SBUX\n",
      "SCHW\n",
      "Error: string indices must be integers\n",
      "SO\n",
      "Already have SO\n",
      "SPG\n",
      "Already have SPG\n",
      "T\n",
      "Already have T\n",
      "TGT\n",
      "Already have TGT\n",
      "TMO\n",
      "Already have TMO\n",
      "TMUS\n",
      "Error: string indices must be integers\n",
      "TSLA\n",
      "Error: string indices must be integers\n",
      "TXN\n",
      "Already have TXN\n",
      "UNH\n",
      "Already have UNH\n",
      "UNP\n",
      "Already have UNP\n",
      "UPS\n",
      "Already have UPS\n",
      "USB\n",
      "Already have USB\n",
      "V\n",
      "Already have V\n",
      "VZ\n",
      "Already have VZ\n",
      "WFC\n",
      "Already have WFC\n",
      "WMT\n",
      "Already have WMT\n",
      "XOM\n",
      "Already have XOM\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "drop() takes from 1 to 2 positional arguments but 3 positional arguments (and 1 keyword-only argument) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bef0915a2eb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mmain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SandP100_joined_closes.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m \u001b[0mcompile_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvisualize_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SandP100_joined_closes.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-bef0915a2eb5>\u001b[0m in \u001b[0;36mcompile_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Adj Close'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mticker\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Open'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'High'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Low'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Volume'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: drop() takes from 1 to 2 positional arguments but 3 positional arguments (and 1 keyword-only argument) were given"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "def save_SandP100_tickers():\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/S%26P_100')\n",
    "    soup = bs.BeautifulSoup(resp.text, \"lxml\")\n",
    "    table = soup.find('table', {'class':'wikitable sortable','id': 'constituents'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    with open(\"SandP100tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers, f)\n",
    "\n",
    "    print(tickers)\n",
    "\n",
    "    return tickers\n",
    "\n",
    "save_SandP100_tickers()\n",
    "\n",
    "def get_data_from_yahoo(reload_SandP100=False):\n",
    "    if reload_SandP100:\n",
    "        tickers = save_SandP100_tickers()\n",
    "    else:\n",
    "        with open(\"SandP100tickers.pickle\",\"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    \n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2022,1,1)\n",
    "    end = dt.datetime.now()\n",
    "\n",
    "    for ticker in tickers:\n",
    "        \n",
    "        print(ticker)\n",
    "        \n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            try:\n",
    "                df = web.DataReader(ticker, 'yahoo', start, end)\n",
    "                df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "            except Exception as ex:\n",
    "                print('Error:', ex)\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "            \n",
    "get_data_from_yahoo()            \n",
    "\n",
    "def compile_data():\n",
    "    with open(\"SandP100tickers.pickle\",\"rb\") as f:\n",
    "        tickers = pickle.load(f)\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for count,ticker in enumerate(tickers):\n",
    "        df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        df.rename(columns = {'Adj Close': ticker}, inplace=True)\n",
    "        df.drop(['Open','High','Low','Close','Volume'], 1, inplace=True)\n",
    "\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df, how='outer')\n",
    "\n",
    "        if count % 10 == 0:\n",
    "            print(count)\n",
    "\n",
    "    print(main_df.head)\n",
    "    main_df.to_csv('SandP100_joined_closes.csv')\n",
    "\n",
    "compile_data()\n",
    "def visualize_data():\n",
    "    df= pd.read_csv('SandP100_joined_closes.csv')\n",
    "    \n",
    "\n",
    "    df_corr = df.corr()\n",
    "    print(df_corr.head())\n",
    "    df_corr.to_csv('SandP100corr.csv')\n",
    "\n",
    "    data=df_corr.values\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    heatmap= ax.pcolor(data, cmap=plt.cm.RdYlGn)\n",
    "    fig.colorbar(heatmap)\n",
    "\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.tick_top()\n",
    "\n",
    "    column_labels = df_corr.columns\n",
    "    row_labels = df_corr.index\n",
    "    ax.set_xticklabels(column_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "    plt.xticks(rotation=90)\n",
    "    heatmap.set_clim(-1,1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "visualize_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The last fuction is best viewed in Python IDLE as the viewer can zoom in and out to make the data visible and usable. Jupyter will not let the user to zoom in and out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"p5\"></a>\n",
    "### Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created a program that downlaods a list of all the tickers from the S&P100 , dowloads ,for each company, the High,Low,Open,Close,Adj Close prices and the volume for each company in the S&P 100. It then makes a csv file with all the Adj Close prices from (1,1,2020) until the day the programme is run into one file. I also added another feauture that shows each companies correlation to the other companies by a heatmap. I had used a lot of Python libraries, some that I had never previously used before.\n",
    "\n",
    "Overall I believe my project works as should and can be very useful to people who are investors or interested in the stock market. I believe my code can be adjusted for many other indexes very easily so makes it more appealing to investors. I have really enjoyed creating my program and have learned the basics of Python really well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from pandas) (2020.4)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: pandas-datareader in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (0.9.0)\n",
      "Collecting pandas-datareader\n",
      "  Downloading pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from pandas-datareader) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from pandas-datareader) (2.25.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from pandas-datareader) (4.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from pandas>=0.23->pandas-datareader) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from pandas>=0.23->pandas-datareader) (2020.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from pandas>=0.23->pandas-datareader) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from pandas>=0.23->pandas-datareader) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.23->pandas-datareader) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from requests>=2.19.0->pandas-datareader) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from requests>=2.19.0->pandas-datareader) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from requests>=2.19.0->pandas-datareader) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\fahed\\downloads\\jupyter\\envs\\project\\lib\\site-packages (from requests>=2.19.0->pandas-datareader) (3.0.4)\n",
      "Installing collected packages: pandas-datareader\n",
      "  Attempting uninstall: pandas-datareader\n",
      "    Found existing installation: pandas-datareader 0.9.0\n",
      "    Uninstalling pandas-datareader-0.9.0:\n",
      "      Successfully uninstalled pandas-datareader-0.9.0\n",
      "Successfully installed pandas-datareader-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pandas\n",
    "!pip3 install --upgrade pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'AIG', 'AMD', 'AMGN', 'AMT', 'AMZN', 'AVGO', 'AXP', 'BA', 'BAC', 'BK', 'BKNG', 'BLK', 'BMY', 'BRK.B', 'C', 'CAT', 'CHTR', 'CL', 'CMCSA', 'COF', 'COP', 'COST', 'CRM', 'CSCO', 'CVS', 'CVX', 'DE', 'DHR', 'DIS', 'DOW', 'DUK', 'EMR', 'EXC', 'F', 'FDX', 'GD', 'GE', 'GILD', 'GM', 'GOOG', 'GOOGL', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KHC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'MA', 'MCD', 'MDLZ', 'MDT', 'MET', 'META', 'MMM', 'MO', 'MRK', 'MS', 'MSFT', 'NEE', 'NFLX', 'NKE', 'NVDA', 'ORCL', 'PEP', 'PFE', 'PG', 'PM', 'PYPL', 'QCOM', 'RTX', 'SBUX', 'SCHW', 'SO', 'SPG', 'T', 'TGT', 'TMO', 'TMUS', 'TSLA', 'TXN', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VZ', 'WFC', 'WMT', 'XOM']\n",
      "AAPL\n",
      "Error: string indices must be integers\n",
      "ABBV\n",
      "Error: string indices must be integers\n",
      "ABT\n",
      "Error: string indices must be integers\n",
      "ACN\n",
      "Error: string indices must be integers\n",
      "ADBE\n",
      "Error: string indices must be integers\n",
      "AIG\n",
      "Error: string indices must be integers\n",
      "AMD\n",
      "Error: string indices must be integers\n",
      "AMGN\n",
      "Error: string indices must be integers\n",
      "AMT\n",
      "Error: string indices must be integers\n",
      "AMZN\n",
      "Error: string indices must be integers\n",
      "AVGO\n",
      "Error: string indices must be integers\n",
      "AXP\n",
      "Error: string indices must be integers\n",
      "BA\n",
      "Error: string indices must be integers\n",
      "BAC\n",
      "Error: string indices must be integers\n",
      "BK\n",
      "Error: string indices must be integers\n",
      "BKNG\n",
      "Error: string indices must be integers\n",
      "BLK\n",
      "Error: string indices must be integers\n",
      "BMY\n",
      "Error: string indices must be integers\n",
      "BRK.B\n",
      "Error: string indices must be integers\n",
      "C\n",
      "Error: string indices must be integers\n",
      "CAT\n",
      "Error: string indices must be integers\n",
      "CHTR\n",
      "Error: string indices must be integers\n",
      "CL\n",
      "Error: string indices must be integers\n",
      "CMCSA\n",
      "Error: string indices must be integers\n",
      "COF\n",
      "Error: string indices must be integers\n",
      "COP\n",
      "Error: string indices must be integers\n",
      "COST\n",
      "Error: string indices must be integers\n",
      "CRM\n",
      "Error: string indices must be integers\n",
      "CSCO\n",
      "Error: string indices must be integers\n",
      "CVS\n",
      "Error: string indices must be integers\n",
      "CVX\n",
      "Error: string indices must be integers\n",
      "DE\n",
      "Error: string indices must be integers\n",
      "DHR\n",
      "Error: string indices must be integers\n",
      "DIS\n",
      "Error: string indices must be integers\n",
      "DOW\n",
      "Error: string indices must be integers\n",
      "DUK\n",
      "Error: string indices must be integers\n",
      "EMR\n",
      "Error: string indices must be integers\n",
      "EXC\n",
      "Error: string indices must be integers\n",
      "F\n",
      "Error: string indices must be integers\n",
      "FDX\n",
      "Error: string indices must be integers\n",
      "GD\n",
      "Error: string indices must be integers\n",
      "GE\n",
      "Error: string indices must be integers\n",
      "GILD\n",
      "Error: string indices must be integers\n",
      "GM\n",
      "Error: string indices must be integers\n",
      "GOOG\n",
      "Error: string indices must be integers\n",
      "GOOGL\n",
      "Error: string indices must be integers\n",
      "GS\n",
      "Error: string indices must be integers\n",
      "HD\n",
      "Error: string indices must be integers\n",
      "HON\n",
      "Error: string indices must be integers\n",
      "IBM\n",
      "Error: string indices must be integers\n",
      "INTC\n",
      "Error: string indices must be integers\n",
      "JNJ\n",
      "Error: string indices must be integers\n",
      "JPM\n",
      "Error: string indices must be integers\n",
      "KHC\n",
      "Error: string indices must be integers\n",
      "KO\n",
      "Error: string indices must be integers\n",
      "LIN\n",
      "Error: string indices must be integers\n",
      "LLY\n",
      "Error: string indices must be integers\n",
      "LMT\n",
      "Error: string indices must be integers\n",
      "LOW\n",
      "Error: string indices must be integers\n",
      "MA\n",
      "Error: string indices must be integers\n",
      "MCD\n",
      "Error: string indices must be integers\n",
      "MDLZ\n",
      "Error: string indices must be integers\n",
      "MDT\n",
      "Error: string indices must be integers\n",
      "MET\n",
      "Error: string indices must be integers\n",
      "META\n",
      "Error: string indices must be integers\n",
      "MMM\n",
      "Error: string indices must be integers\n",
      "MO\n",
      "Error: string indices must be integers\n",
      "MRK\n",
      "Error: string indices must be integers\n",
      "MS\n",
      "Error: string indices must be integers\n",
      "MSFT\n",
      "Error: string indices must be integers\n",
      "NEE\n",
      "Error: string indices must be integers\n",
      "NFLX\n",
      "Error: string indices must be integers\n",
      "NKE\n",
      "Error: string indices must be integers\n",
      "NVDA\n",
      "Error: string indices must be integers\n",
      "ORCL\n",
      "Error: string indices must be integers\n",
      "PEP\n",
      "Error: string indices must be integers\n",
      "PFE\n",
      "Error: string indices must be integers\n",
      "PG\n",
      "Error: string indices must be integers\n",
      "PM\n",
      "Error: string indices must be integers\n",
      "PYPL\n",
      "Error: string indices must be integers\n",
      "QCOM\n",
      "Error: string indices must be integers\n",
      "RTX\n",
      "Error: string indices must be integers\n",
      "SBUX\n",
      "Error: string indices must be integers\n",
      "SCHW\n",
      "Error: string indices must be integers\n",
      "SO\n",
      "Error: string indices must be integers\n",
      "SPG\n",
      "Error: string indices must be integers\n",
      "T\n",
      "Error: string indices must be integers\n",
      "TGT\n",
      "Error: string indices must be integers\n",
      "TMO\n",
      "Error: string indices must be integers\n",
      "TMUS\n",
      "Error: string indices must be integers\n",
      "TSLA\n",
      "Error: string indices must be integers\n",
      "TXN\n",
      "Error: string indices must be integers\n",
      "UNH\n",
      "Error: string indices must be integers\n",
      "UNP\n",
      "Error: string indices must be integers\n",
      "UPS\n",
      "Error: string indices must be integers\n",
      "USB\n",
      "Error: string indices must be integers\n",
      "V\n",
      "Error: string indices must be integers\n",
      "VZ\n",
      "Error: string indices must be integers\n",
      "WFC\n",
      "Error: string indices must be integers\n",
      "WMT\n",
      "Error: string indices must be integers\n",
      "XOM\n",
      "Error: string indices must be integers\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stock_dfs\\\\AAPL.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-127bccc57297>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mmain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SandP100_joined_closes.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[0mcompile_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-127bccc57297>\u001b[0m in \u001b[0;36mcompile_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mticker\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stock_dfs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Jupyter\\envs\\project\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Jupyter\\envs\\project\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Jupyter\\envs\\project\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Jupyter\\envs\\project\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1661\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Jupyter\\envs\\project\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stock_dfs\\\\AAPL.csv'"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas_datareader.data as web\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def save_SandP100_tickers():\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/S%26P_100')\n",
    "    soup = bs.BeautifulSoup(resp.text, \"lxml\")\n",
    "    table = soup.find('table', {'class':'wikitable sortable','id': 'constituents'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text.strip()\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    with open(\"SandP100tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers, f)\n",
    "\n",
    "    print(tickers)\n",
    "\n",
    "    return tickers\n",
    "\n",
    "save_SandP100_tickers()\n",
    "\n",
    "def get_data_from_yahoo(reload_SandP100=False):\n",
    "    if reload_SandP100:\n",
    "        tickers = save_SandP100_tickers()\n",
    "    else:\n",
    "        with open(\"SandP100tickers.pickle\",\"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    \n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2021, 1, 1)\n",
    "    end = dt.datetime.now()\n",
    "\n",
    "    for ticker in tickers:\n",
    "        \n",
    "        print(ticker)\n",
    "        \n",
    "        if not os.path.exists(os.path.join('stock_dfs', '{}.csv'.format(ticker))):\n",
    "            try:\n",
    "                df = web.DataReader(ticker, 'yahoo', start, end)\n",
    "                df.to_csv(os.path.join('stock_dfs', '{}.csv'.format(ticker)))\n",
    "            except Exception as ex:\n",
    "                print('Error:', ex)\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "            \n",
    "get_data_from_yahoo()            \n",
    "\n",
    "def compile_data():\n",
    "    with open(\"SandP100tickers.pickle\",\"rb\") as f:\n",
    "        tickers = pickle.load(f)\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for count,ticker in enumerate(tickers):\n",
    "        df = pd.read_csv(os.path.join('stock_dfs', '{}.csv'.format(ticker)))\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        df.rename(columns={'Adj Close': ticker}, inplace=True)\n",
    "        df.drop(['Open','High','Low','Close','Volume'], axis=1, inplace=True)  # Corrected line\n",
    "\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df, how='outer')\n",
    "\n",
    "        if count % 10 == 0:\n",
    "            print(count)\n",
    "\n",
    "    print(main_df.head())\n",
    "    main_df.to_csv('SandP100_joined_closes.csv')\n",
    "\n",
    "compile_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
